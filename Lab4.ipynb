{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIE424 (2022 Winter) Lab 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using JAX to calculate gradients\n",
    "======\n",
    "\n",
    "This part of the lab will explain how to automatically calculate gradients that can then be used in many optimiazation algorithms, such as gradient descent.\n",
    "\n",
    "* The hope is to give you a base to be able to implement algorithms manually.\n",
    "* Visualizing these concepts makes life much easier.\n",
    "* Get into the habit of trying things out! Machine learning is wonderful because it is so successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)\n",
    "\n",
    "data_points, data_dimension = 100, 2\n",
    "\n",
    "# Generate X and w, then set y = Xw + Ïµ\n",
    "X = random.normal(key, (data_points, data_dimension))\n",
    "\n",
    "true_w = random.normal(key, (data_dimension,))\n",
    "y = X.dot(true_w) + 0.1 * random.normal(key, (data_points,))\n",
    "\n",
    "def make_squared_error(X, y):\n",
    "    def squared_error(w):\n",
    "        return jnp.sum(jnp.power(X.dot(w) - y, 2)) / X.shape[0]\n",
    "    return squared_error\n",
    "\n",
    "# Now use jax grad\n",
    "grad_loss = grad(make_squared_error(X, y))\n",
    "\n",
    "# Basic gradient descent\n",
    "w_grad = np.zeros(data_dimension)\n",
    "step = 0.1\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    w_grad = w_grad - step * grad_loss(w_grad)\n",
    "\n",
    "# Linear algebra solution\n",
    "w_linalg = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_grad)\n",
    "print(w_linalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite Difference Estimation\n",
    "\n",
    "One way to estimate the gradient at a certain point is to take the \"slope\" at that point with a very small step in x.\n",
    "Let's check if the grad of jax is accurate and makes sense numerically using finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a step size for finite differences calculations\n",
    "eps = 1e-2\n",
    "w = true_w\n",
    "\n",
    "# Check W_grad with finite differences in a random direction\n",
    "key, subkey = random.split(key)\n",
    "vec = random.normal(subkey, w.shape)\n",
    "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
    "w_grad_numerical = (make_squared_error(X, y)(w + eps / 2. * unitvec) - make_squared_error(X, y)(w - eps / 2. * unitvec)) / eps\n",
    "print('w_grad_numerical', w_grad_numerical)\n",
    "print('W_dirderiv_autodiff', jnp.vdot(grad(make_squared_error(X, y))(w), unitvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in np.logspace(-8,0,9):\n",
    "    w_grad_numerical = (make_squared_error(X, y)(w + eps / 2. * unitvec) - make_squared_error(X, y)(w - eps / 2. * unitvec)) / eps\n",
    "    print(f'Epsilon used: {eps}, directional derivative with FDM:  {w_grad_numerical}, difference with autodiff: {abs(w_grad_numerical-jnp.vdot(grad(make_squared_error(X, y))(w), unitvec))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Gradient Descent\n",
    "\n",
    "Now we are going to talk about visualizing each step in the gradient descent algorithm, starting with a one dimensional, and then continuing with a two dimensional data. We will be looking at how this affects both the data space (X vs Y) and the parameter space (w0 vs w1 vs Loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The data to fit\n",
    "m = 20\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta1_true * x + 0.02*np.random.randn(m)\n",
    "\n",
    "# The plot: LHS is the data, RHS will be the cost function.\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,6.15))\n",
    "ax[0].scatter(x, y, marker='x', s=40, color='k')\n",
    "\n",
    "def cost_func(theta1):\n",
    "    \"\"\"The cost function, J(theta1) describing the goodness of fit.\"\"\"\n",
    "    theta1 = np.atleast_2d(np.asarray(theta1))\n",
    "    return np.average((y-hypothesis(x, theta1))**2, axis=1)/2\n",
    "\n",
    "def hypothesis(x, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line through the origin.\"\"\"\n",
    "    return theta1*x\n",
    "\n",
    "# First construct a grid of theta1 parameter pairs and their corresponding\n",
    "# cost function values.\n",
    "theta1_grid = np.linspace(-0.2,1,50)\n",
    "J_grid = cost_func(theta1_grid[:,np.newaxis])\n",
    "\n",
    "# The cost function as a function of its single parameter, theta1.\n",
    "ax[1].plot(theta1_grid, J_grid, 'k')\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at theta1 = 0.\n",
    "N = 5\n",
    "alpha = 1\n",
    "theta1 = [0]\n",
    "J = [cost_func(theta1[0])[0]]\n",
    "for j in range(N-1):\n",
    "    last_theta1 = theta1[-1]\n",
    "    this_theta1 = last_theta1 - alpha / m * np.sum(\n",
    "                                    (hypothesis(x, last_theta1) - y) * x)\n",
    "    theta1.append(this_theta1)\n",
    "    J.append(cost_func(this_theta1))\n",
    "\n",
    "# Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the LHS data plot in a matching colour.\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "ax[0].plot(x, hypothesis(x, theta1[0]), color=colors[0], lw=2,\n",
    "           label=r'$\\theta_1 = {:.3f}$'.format(theta1[0]))\n",
    "for j in range(1,N):\n",
    "    ax[1].annotate('', xy=(theta1[j], J[j]), xytext=(theta1[j-1], J[j-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    ax[0].plot(x, hypothesis(x, theta1[j]), color=colors[j], lw=2,\n",
    "               label=r'$\\theta_1 = {:.3f}$'.format(theta1[j]))\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "ax[1].scatter(theta1, J, c=colors, s=40, lw=0)\n",
    "ax[1].set_xlim(-0.2,1)\n",
    "ax[1].set_xlabel(r'$\\theta_1$')\n",
    "ax[1].set_ylabel(r'$J(\\theta_1)$')\n",
    "ax[1].set_title('Cost function')\n",
    "ax[0].set_xlabel(r'$x$')\n",
    "ax[0].set_ylabel(r'$y$')\n",
    "ax[0].set_title('Data and fit')\n",
    "ax[0].legend(loc='upper left', fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data to fit\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x + 0.02*np.random.randn(m)\n",
    "\n",
    "# The plot: LHS is the data, RHS will be the cost function.\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,6.15))\n",
    "ax[0].scatter(x, y, marker='x', s=40, color='k')\n",
    "\n",
    "def cost_func(theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    theta0 = np.atleast_3d(np.asarray(theta0))\n",
    "    theta1 = np.atleast_3d(np.asarray(theta1))\n",
    "    return np.average((y-hypothesis(x, theta0, theta1))**2, axis=2)/2\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    return theta0 + theta1*x\n",
    "\n",
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "theta0_grid = np.linspace(-1,4,101)\n",
    "theta1_grid = np.linspace(-5,5,101)\n",
    "J_grid = cost_func(theta0_grid[np.newaxis,:,np.newaxis],\n",
    "                   theta1_grid[:,np.newaxis,np.newaxis])\n",
    "\n",
    "# A labeled contour plot for the RHS cost function\n",
    "X, Y = np.meshgrid(theta0_grid, theta1_grid)\n",
    "contours = ax[1].contour(X, Y, J_grid, 30)\n",
    "ax[1].clabel(contours)\n",
    "# The target parameter values indicated on the cost function contour plot\n",
    "ax[1].scatter([theta0_true]*2,[theta1_true]*2,s=[50,10], color=['k','w'])\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at (theta0, theta1) = (0, 0).\n",
    "N = 5\n",
    "alpha = 0.7\n",
    "theta = [np.array((0,0))]\n",
    "J = [cost_func(*theta[0])[0]]\n",
    "for j in range(N-1):\n",
    "    last_theta = theta[-1]\n",
    "    this_theta = np.empty((2,))\n",
    "    this_theta[0] = last_theta[0] - alpha / m * np.sum(\n",
    "                                    (hypothesis(x, *last_theta) - y))\n",
    "    this_theta[1] = last_theta[1] - alpha / m * np.sum(\n",
    "                                    (hypothesis(x, *last_theta) - y) * x)\n",
    "    theta.append(this_theta)\n",
    "    J.append(cost_func(*this_theta))\n",
    "\n",
    "\n",
    "# Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the LHS data plot in a matching colour.\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "ax[0].plot(x, hypothesis(x, *theta[0]), color=colors[0], lw=2,\n",
    "           label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[0]))\n",
    "for j in range(1,N):\n",
    "    ax[1].annotate('', xy=theta[j], xytext=theta[j-1],\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    ax[0].plot(x, hypothesis(x, *theta[j]), color=colors[j], lw=2,\n",
    "           label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[j]))\n",
    "ax[1].scatter(*zip(*theta), c=colors, s=40, lw=0)\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "ax[1].set_xlabel(r'$\\theta_0$')\n",
    "ax[1].set_ylabel(r'$\\theta_1$')\n",
    "ax[1].set_title('Cost function')\n",
    "ax[0].set_xlabel(r'$x$')\n",
    "ax[0].set_ylabel(r'$y$')\n",
    "ax[0].set_title('Data and fit')\n",
    "axbox = ax[0].get_position()\n",
    "# Position the legend by hand so that it doesn't cover up any of the lines.\n",
    "ax[0].legend(loc=(axbox.x0+0.5*axbox.width, axbox.y0+0.1*axbox.height),\n",
    "             fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Code was used from the following sources:\n",
    "\n",
    "https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/autodiff_cookbook.ipynb#scrollTo=R8q5RiY3l7Fw\n",
    "\n",
    "https://scipython.com/blog/visualizing-the-gradient-descent-method/\n",
    "\n",
    "https://colindcarroll.com/2019/04/06/exercises-in-automatic-differentiation-using-autograd-and-jax/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
